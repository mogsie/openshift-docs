include::modules/serverless-document-attributes.adoc[]
[id="serverless-kafka"]
= Using Apache Kafka with {ServerlessProductName}
include::modules/common-attributes.adoc[]
:context: serverless-kafka

toc::[]

:FeatureName: Apache Kafka on {ServerlessProductName}
include::modules/technology-preview.adoc[leveloffset=+2]

You can use the `KafkaChannel` channel type and `KafkaSource` event source with {ServerlessProductName}.
To do this, you must install the Knative Kafka components, and configure the integration between {ServerlessProductName} and a supported link:https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html/amq_streams_on_openshift_overview/index[Red Hat AMQ Streams] cluster.

The {ServerlessOperatorName} provides the Knative Kafka API that can be used to create a `KnativeKafka` custom resource:

.Example `KnativeKafka` custom resource
[source,yaml]
----
apiVersion: operator.serverless.openshift.io/v1alpha1
kind: KnativeKafka
metadata:
    name: knative-kafka
    namespace: knative-eventing
spec:
    channel:
        enabled: true <1>
        bootstrapServers: <bootstrap_server> <2>
    source:
        enabled: true <3>
----
<1> Enables developers to use the `KafkaChannel` channel type in the cluster.
<2> A comma-separated list of bootstrap servers from your AMQ Streams cluster.
<3> Enables developers to use the `KafkaSource` event source type in the cluster.

// Install Kafka
include::modules/serverless-install-kafka-odc.adoc[leveloffset=+1]

[id="serverless-kafka-next-steps"]
== Next steps

* Create a xref:../serverless/event_sources/serverless-kafka-source.adoc#serverless-kafka-source[Kafka event source].
* Create a xref:../serverless/event_workflows/serverless-channels.adoc#serverless-channels[Kafka channel].
